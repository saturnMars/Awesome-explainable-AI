# Uncategorized Papers on Model/Instance Explanation

[CONCEPT BOTTLENECK LARGE LANGUAGE MODELS](https://openreview.net/pdf?id=RC5FPYVQaH), ICLR 2025

[TOWARD UNDERSTANDING IN-CONTEXT VS. INWEIGHT LEARNING](https://openreview.net/pdf?id=aKJr5NnN8U), ICLR 2025

[Paths, Proofs, and Perfection: Developing a Human-Interpretable Proof System for Constrained Shortest Paths ](https://ojs.aaai.org/index.php/AAAI/article/view/30068), AAAI 2024

[Stratified GNN Explanations through Sufficient Expansion](https://ojs.aaai.org/index.php/AAAI/article/view/29180), AAAI 2024

[Keep the Faith: Faithful Explanations in Convolutional Neural Networks for Case-Based Reasoning](https://ojs.aaai.org/index.php/AAAI/article/view/28406/28794), AAAI 2024

[Explaining Generalization Power of a DNN using Interactive Concepts](https://ojs.aaai.org/index.php/AAAI/article/view/29655), AAAI 2024

[Accelerating the Global Aggregation of Local Explanations](https://ojs.aaai.org/index.php/AAAI/article/view/29845/31471), AAAI 2024

[Statistically Signifcant Concept-based Explanation of Image Classifers via Model Knockoffs](https://www.ijcai.org/proceedings/2023/0058.pdf), IJCAI 2023

[Towards Relatable Explainable AI with the Perceptual Process](https://arxiv.org/pdf/2112.14005.pdf), CHI 2022 Best paper

[A Psychological Theory of Explainability](https://arxiv.org/pdf/2205.08452.pdf), ICML 2022

[CEBaB: Estimating the Causal Effects of Real-World Concepts on NLP Model Behavior](https://arxiv.org/abs/2205.14140), NeurIPS 2022

[On the Safety of Interpretable Machine Learning: A Maximum Deviation Approach](https://openreview.net/pdf?id=WPXRVQaP9Oq), NeurIPS 2022

[Concept Activation Regions: A Generalized Framework For Concept-Based Explanations](https://openreview.net/pdf?id=8AB7AXaLIX5), NeurIPS 2022

[The Conflict Between Explainable and Accountable Decision-Making Algorithms](https://facctconference.org/static/pdfs_2022/facct22-162.pdf), FAccT 2022

[Post-Hoc Explanations Fail to Achieve their Purpose in Adversarial Contexts](https://facctconference.org/static/pdfs_2022/facct22-73.pdf), FAccT 2022

[The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations](https://facctconference.org/static/pdfs_2022/facct22-97.pdf), FAccT 2022

[How Explainability Contributes to Trust in AI](https://facctconference.org/static/pdfs_2022/facct22-117.pdf), FAccT 2022

[“There Is Not Enough Information”: On the Effects of Explanations on Perceptions of Informational Fairness and Trustworthiness in Automated Decision-Making](https://facctconference.org/static/pdfs_2022/facct22-130.pdf), FaccT 2022

[InterpretDL: Explaining Deep Models in PaddlePaddle](https://jmlr.org/papers/volume23/21-0738/21-0738.pdf), JMLR 202

[Constraint-Driven Explanations for Black Box ML Models](https://www.comp.nus.edu.sg/~meel/Papers/aaai22-snimsv.pdf), AAAI 2022

[The Utility of Explainable AI in Ad Hoc Human-Machine Teaming](https://proceedings.neurips.cc/paper/2021/file/05d74c48b5b30514d8e9bd60320fc8f6-Paper.pdf), NeurIPS 2021

[On Completeness-aware Concept-Based Explanations in Deep Neural Networks](https://proceedings.neurips.cc/paper/2020/file/ecb287ff763c169694f682af52c1f309-Paper.pdf), NeurIPS 2020

[Local Explanation of Dialogue Response Generation](https://arxiv.org/abs/2106.06528), NeurIPS 2021

[Improving Deep Learning Interpretability by Saliency Guided Training](https://openreview.net/pdf?id=x4zs7eC-BsI), NeurIPS 2021

[Explaining Hyperparameter Optimization via Partial Dependence Plots](https://arxiv.org/abs/2111.04820), NeurIPS 2021

[Learning Groupwise Explanations for Black-Box Models](https://www.ijcai.org/proceedings/2021/0330.pdf), IJCAI 2021

[On Explaining Random Forests with SAT](https://www.ijcai.org/proceedings/2021/0356.pdf), IJCAI 2021

[What Changed? Interpretable Model Comparison](https://www.ijcai.org/proceedings/2021/0393.pdf), IJCAI 2021

[Towards Probabilistic Sufficient Explanations](http://starai.cs.ucla.edu/papers/WangXXAI20.pdf), IJCAI 2021

[On Explainability of Graph Neural Networks via Subgraph Explorations](https://arxiv.org/pdf/2102.05152.pdf), ICML 2021

[Why Attentions May Not Be Interpretable?](https://arxiv.org/abs/2006.05656), KDD 2021

[Where and What? Examining Interpretable Disentangled Representations](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhu_Where_and_What_Examining_Interpretable_Disentangled_Representations_CVPR_2021_paper.pdf), CVPR 2021

[Verifiability and Predictability: Interpreting Utilities of Network Architectures
for Point Cloud Processing](https://openaccess.thecvf.com/content/CVPR2021/papers/Shen_Verifiability_and_Predictability_Interpreting_Utilities_of_Network_Architectures_for_Point_CVPR_2021_paper.pdf), CVPR 2021

[Have We Learned to Explain?: How Interpretability Methods Can Learn to Encode Predictions in their Interpretations](https://arxiv.org/abs/2103.01890), AISTATS 2021

[Does Explainable Artificial Intelligence Improve Human Decision-Making?](https://arxiv.org/abs/2006.11194), AAAI 2021

[Robustness in machine learning explanations: does it matter?](https://dl.acm.org/doi/abs/10.1145/3351095.3372836), FAccT 2020

[Incorporating Interpretable Output Constraints in Bayesian Neural Networks](https://arxiv.org/abs/2010.10969), NeuIPS 2020

[Towards Interpretable Natural Language Understanding with Explanations as Latent Variables](https://arxiv.org/abs/2011.05268), NeurIPS 2020

[Learning Deep Attribution Priors Based On Prior Knowledge](https://arxiv.org/abs/1912.10065), NeurIPS 2020

[Understanding Global Feature Contributions through Additive Importance Measures](https://arxiv.org/abs/2004.00668), NeurIPS 2020

[Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE](https://arxiv.org/abs/2011.04798), NeurIPS 2020

[Generative causal explanations of black-box classifiers](https://arxiv.org/abs/2006.13913), NeurIPS 2020

[Learning outside the Black-Box: The pursuit of interpretable models](https://proceedings.neurips.cc//paper/2020/file/ce758408f6ef98d7c7a7b786eca7b3a8-Paper.pdf), NeurIPS 2020

[Explaining Groups of Points in Low-Dimensional Representations](https://arxiv.org/pdf/2003.01640.pdf), ICML 2020

[Explaining Knowledge Distillation by Quantifying the Knowledge](https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Explaining_Knowledge_Distillation_by_Quantifying_the_Knowledge_CVPR_2020_paper.pdf), CVPR 2020

[Principles of Explanation in Human-AI Systems](https://arxiv.org/pdf/2102.04972.pdf), AAAI 2020

[Fanoos: Multi-Resolution, Multi-Strength, Interactive Explanations for Learned Systems](https://arxiv.org/pdf/2006.12453.pdf), IJCAI 2020

[Machine Learning Explainability for External Stakeholders](https://arxiv.org/abs/2007.05408), IJCAI 2020

[Py-CIU: A Python Library for Explaining Machine Learning Predictions Using Contextual Importance and Utility](https://www.researchgate.net/publication/344154017_Py-CIU_A_Python_Library_for_Explaining_Machine_Learning_Predictions_Using_Contextual_Importance_and_Utility), IJCAI 2020

[Machine Learning Explainability for External Stakeholders](https://arxiv.org/abs/2007.05408), IJCAI 2020

[Interpretable Models for Understanding Immersive Simulations](https://www.ijcai.org/Proceedings/2020/0321.pdf), IJCAI 2020

[Towards Automatic Concept-based Explanations](https://arxiv.org/pdf/1902.03129.pdf), NIPS 2019

[Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead](https://arxiv.org/pdf/1811.10154.pdf), Nature Machine Intelligence 2019

[Interpretml: A unified framework for machine learning interpretability](https://arxiv.org/abs/1909.09223), arxiv preprint 2019

[All Models are Wrong, but Many are Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models Simultaneously](https://arxiv.org/pdf/1801.01489.pdf), JMLR 2019

[On the Robustness of Interpretability Methods](https://arxiv.org/abs/1806.08049), ICML 2018 workshop

[Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in
Deep Neural Networks](https://openaccess.thecvf.com/content_cvpr_2018/papers/Fong_Net2Vec_Quantifying_and_CVPR_2018_paper.pdf), CVPR 2018


[Towards A Rigorous Science of Interpretable Machine Learning](https://arxiv.org/pdf/1702.08608.pdf), Arxiv preprint 2017

[Object Region Mining With Adversarial Erasing: A Simple Classification to Semantic Segmentation Approach](https://openaccess.thecvf.com/content_cvpr_2017/papers/Wei_Object_Region_Mining_CVPR_2017_paper.pdf), CVPR 2017

LOCO, [Distribution-Free Predictive Inference For Regression](https://arxiv.org/pdf/1604.04173.pdf), Arxiv preprint 2016


[Explaining data-driven document classifications](https://www.jstor.org/stable/26554869), MIS Quarterly 2014

